[1mdiff --git a/sparch/exp.py b/sparch/exp.py[m
[1mindex 5e9cc5c..c124f18 100644[m
[1m--- a/sparch/exp.py[m
[1m+++ b/sparch/exp.py[m
[36m@@ -86,6 +86,8 @@[m [mclass Experiment:[m
         self.use_augm = config.pop('use_augm')[m
         self.s4_opt = config.pop('s4_opt')[m
 [m
[32m+[m[32m        self.workers = config.pop('num_workers')[m
[32m+[m
         self.nb_steps = config.pop('nb_steps')[m
         self.max_time = config.pop('max_time')[m
         self.spatial_bin = config.pop('spatial_bin')[m
[36m@@ -294,7 +296,7 @@[m [mclass Experiment:[m
                 max_time = self.max_time,[m
                 spatial_bin = self.spatial_bin,[m
                 shuffle=True,[m
[31m-                workers=8,[m
[32m+[m[32m                workers=self.workers,[m
             )[m
             self.valid_loader = load_shd_or_ssc([m
                 dataset_name=self.dataset_name,[m
[36m@@ -305,7 +307,7 @@[m [mclass Experiment:[m
                 max_time = self.max_time,[m
                 spatial_bin = self.spatial_bin,[m
                 shuffle=False,[m
[31m-                workers=8,[m
[32m+[m[32m                workers=self.workers,[m
             )[m
             if self.dataset_name == "ssc":[m
                 self.test_loader = load_shd_or_ssc([m
[36m@@ -316,7 +318,7 @@[m [mclass Experiment:[m
                     nb_steps=self.nb_steps,[m
                     max_time = self.max_time,[m
                     shuffle=False,[m
[31m-                    workers=8,[m
[32m+[m[32m                    workers=self.workers,[m
                 )[m
             if self.use_augm:[m
                 logging.warning([m
[36m@@ -336,7 +338,7 @@[m [mclass Experiment:[m
                 batch_size=self.batch_size,[m
                 use_augm=self.use_augm,[m
                 shuffle=True,[m
[31m-                workers=8,[m
[32m+[m[32m                workers=self.workers,[m
             )[m
             self.valid_loader = load_hd_or_sc([m
                 dataset_name=self.dataset_name,[m
[36m@@ -345,7 +347,7 @@[m [mclass Experiment:[m
                 batch_size=self.batch_size,[m
                 use_augm=self.use_augm,[m
                 shuffle=False,[m
[31m-                workers=8,[m
[32m+[m[32m                workers=self.workers,[m
             )[m
             if self.dataset_name == "sc":[m
                 self.test_loader = load_hd_or_sc([m
[36m@@ -355,7 +357,7 @@[m [mclass Experiment:[m
                     batch_size=self.batch_size,[m
                     use_augm=self.use_augm,[m
                     shuffle=False,[m
[31m-                    workers=8,[m
[32m+[m[32m                    workers=self.workers,[m
                 )[m
             if self.use_augm:[m
                 logging.info("\nData augmentation is used\n")[m
[36m@@ -375,7 +377,7 @@[m [mclass Experiment:[m
             self.net = torch.load(self.load_path, map_location=self.device)[m
             logging.info(f"\nLoaded model at: {self.load_path}\n {self.net}\n")[m
 [m
[31m-        elif self.model_type in ["LIF", "LIFfeature", "adLIFnoClamp", "LIFfeatureDim", "adLIF", "CadLIF", "ResonateFire", "BRF", "RSEadLIF", "adLIFclamp", "RLIF", "RadLIF", "LIFcomplex","LIFcomplexBroad", "LIFrealcomplex", "ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlphaNoB","RLIFcomplex1MinAlpha", "LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"]:[m
[32m+[m[32m        elif self.model_type in ["LIF", "LIFfeature", "adLIFnoClamp", "LIFfeatureDim", "adLIF", "CadLIF", "ResonateFire", "RAFAblation", "BRF", "RSEadLIF", "adLIFclamp", "RLIF", "RadLIF", "LIFcomplex","LIFcomplexBroad", "LIFrealcomplex", "ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlphaNoB","RLIFcomplex1MinAlpha", "LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"]:[m
 [m
             self.net = SNN([m
                 input_shape=input_shape,[m
[1mdiff --git a/sparch/models/snns.py b/sparch/models/snns.py[m
[1mindex 160a546..e32b01f 100644[m
[1m--- a/sparch/models/snns.py[m
[1m+++ b/sparch/models/snns.py[m
[36m@@ -155,7 +155,7 @@[m [mclass SNN(nn.Module):[m
 [m
         self.extra_features = extra_features[m
 [m
[31m-        if neuron_type not in ["LIF", "adLIF", "CadLIF", "BRF", "ResonateFire", "RSEadLIF", "LIFfeature", "adLIFnoClamp","LIFfeatureDim", "adLIFclamp", "RLIF", "RadLIF", "LIFcomplex", "LIFcomplexBroad", "LIFrealcomplex","ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlphaNoB","RLIFcomplex1MinAlpha", "LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"]:[m
[32m+[m[32m        if neuron_type not in ["LIF", "adLIF", "CadLIF", "RAFAblation", "BRF", "ResonateFire", "RSEadLIF", "LIFfeature", "adLIFnoClamp","LIFfeatureDim", "adLIFclamp", "RLIF", "RadLIF", "LIFcomplex", "LIFcomplexBroad", "LIFrealcomplex","ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlphaNoB","RLIFcomplex1MinAlpha", "LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"]:[m
             raise ValueError(f"Invalid neuron type {neuron_type}")[m
 [m
         # Init trainable parameters[m
[36m@@ -1311,6 +1311,224 @@[m [mclass RSEadLIFLayer(nn.Module):[m
 [m
         return torch.stack(s, dim=1)[m
 [m
[32m+[m[32mclass RAFAblationLayer(nn.Module):[m
[32m+[m[32m    """[m
[32m+[m[32m    A single layer of Leaky Integrate-and-Fire neurons without layer-wise[m
[32m+[m[32m    recurrent connections (LIF).[m
[32m+[m
[32m+[m[32m    Arguments[m
[32m+[m[32m    ---------[m
[32m+[m[32m    input_size : int[m
[32m+[m[32m        Number of features in the input tensors.[m
[32m+[m[32m    hidden_size : int[m
[32m+[m[32m        Number of output neurons.[m
[32m+[m[32m    batch_size : int[m
[32m+[m[32m        Batch size of the input tensors.[m
[32m+[m[32m    threshold : float[m
[32m+[m[32m        Value of spiking threshold (fixed)[m
[32m+[m[32m    dropout : float[m
[32m+[m[32m        Dropout factor (must be between 0 and 1).[m
[32m+[m[32m    normalization : str[m
[32m+[m[32m        Type of normalization. Every string different from 'batchnorm'[m
[32m+[m[32m        and 'layernorm' will result in no normalization.[m
[32m+[m[32m    use_bias : bool[m
[32m+[m[32m        If True, additional trainable bias is used with feedforward weights.[m
[32m+[m[32m    bidirectional : bool[m
[32m+[m[32m        If True, a bidirectional model that scans the sequence both directions[m
[32m+[m[32m        is used, which doubles the size of feedforward matrices in layers l>0.[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    def __init__([m
[32m+[m[32m        self,[m
[32m+[m[32m        input_size,[m
[32m+[m[32m        hidden_size,[m
[32m+[m[32m        batch_size,[m
[32m+[m[32m        threshold=1.0,[m
[32m+[m[32m        dropout=0.0,[m
[32m+[m[32m        normalization="batchnorm",[m
[32m+[m[32m        use_bias=False,[m
[32m+[m[32m        bidirectional=False,[m
[32m+[m[32m        extra_features=None[m
[32m+[m[32m    ):[m
[32m+[m[32m        super().__init__()[m
[32m+[m
[32m+[m[32m        # Fixed parameters[m
[32m+[m[32m        self.input_size = int(input_size)[m
[32m+[m[32m        self.hidden_size = int(hidden_size)[m
[32m+[m[32m        self.batch_size = batch_size[m
[32m+[m[32m        self.threshold = threshold[m
[32m+[m[32m        self.dropout = dropout[m
[32m+[m[32m        self.normalization = normalization[m
[32m+[m[32m        self.use_bias = use_bias[m
[32m+[m[32m        self.bidirectional = bidirectional[m
[32m+[m[32m        self.batch_size = self.batch_size * (1 + self.bidirectional)[m
[32m+[m[41m        [m
[32m+[m[32m        self.spike_fct = SpikeFunctionBoxcar.apply[m
[32m+[m
[32m+[m[32m        # Trainable parameters[m
[32m+[m[32m        self.W = nn.Linear(self.input_size, self.hidden_size, bias=use_bias)[m
[32m+[m
[32m+[m[32m        self.recurrent = extra_features['recurrent'][m
[32m+[m[32m        if self.recurrent:[m
[32m+[m[32m            self.V = nn.Linear(self.hidden_size, self.hidden_size, bias=False)[m
[32m+[m
[32m+[m[32m        self.continuous = extra_features['continuous'][m
[32m+[m[32m        self.reparam = extra_features['reparam'][m
[32m+[m[32m        self.taylor = extra_features['taylor'][m
[32m+[m[32m        self.s4_init = extra_features['s4_init'][m
[32m+[m[32m        self.dt_train = extra_features['dt_train'][m
[32m+[m[41m        [m
[32m+[m[32m        if self.s4_init:[m
[32m+[m[32m            alpha_real = torch.log(0.5 * torch.ones(self.hidden_size))[m
[32m+[m[32m            dt_min = extra_features["dt_min"][m
[32m+[m[32m            dt_max = extra_features["dt_max"][m
[32m+[m[32m            dt = torch.rand(self.hidden_size)*([m
[32m+[m[32m                math.log(dt_max) - math.log(dt_min)[m
[32m+[m[32m            ) + math.log(dt_min)[m
[32m+[m[32m            alpha_im =  math.pi * torch.ones(self.hidden_size)[m
[32m+[m[32m            self.register("alpha_real", alpha_real, lr=0.001)[m
[32m+[m[32m            self.register("dt", dt, lr=0.001)[m
[32m+[m[32m            self.register("alpha_im", alpha_im, lr=0.001)[m
[32m+[m[32m        elif self.reparam:[m[41m [m
[32m+[m[32m            self.dt = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.dt, math.log(0.001), math.log(0.5))[m
[32m+[m[32m            self.alpha_real = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_real, 0.0, math.log(10.0))[m
[32m+[m[32m            self.alpha_im = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_im, 5.0, 10.0)[m
[32m+[m[32m        elif self.dt_train:[m
[32m+[m[32m            self.dt = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.dt, 0.001, 0.4)[m
[32m+[m[32m            self.alpha_real = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_real, 1.0, 10.0)[m
[32m+[m[32m            self.alpha_im = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_im, 5.0, 10.0)[m
[32m+[m[32m        elif self.continuous or self.taylor:[m
[32m+[m[32m            self.dt = 0.004[m
[32m+[m[32m            self.alpha_real = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            self.alpha_im = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_real, 1.0, 10.0)[m
[32m+[m[32m            nn.init.uniform_(self.alpha_im, 5.0, 10.0)[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.dt = 0.004[m
[32m+[m[32m            self.alpha_real = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            self.alpha_im = nn.Parameter(torch.Tensor(self.hidden_size))[m
[32m+[m[32m            nn.init.uniform_(self.alpha_real, 0.7, 0.96)[m
[32m+[m[32m            nn.init.uniform_(self.alpha_im, 0.02, 0.04)[m
[32m+[m
[32m+[m[32m        if extra_features['extra_b']:[m
[32m+[m[32m            self.b = nn.Parameter(torch.rand(self.hidden_size))[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.b = 1.0[m
[32m+[m
[32m+[m[32m        self.threshold = 1.0[m
[32m+[m
[32m+[m[32m        if extra_features['half_reset']:[m
[32m+[m[32m            self.reset_factor = 0.5[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.reset_factor = 1.0[m
[32m+[m
[32m+[m[32m        # Initialize normalinzation[m
[32m+[m[32m        self.normalize = False[m
[32m+[m[32m        if normalization == "batchnorm":[m
[32m+[m[32m            self.norm = nn.BatchNorm1d(self.hidden_size, momentum=0.05)[m
[32m+[m[32m            self.normalize = True[m
[32m+[m[32m        elif normalization == "layernorm":[m
[32m+[m[32m            self.norm = nn.LayerNorm(self.hidden_size)[m
[32m+[m[32m            self.normalize = True[m
[32m+[m
[32m+[m[32m        # Initialize dropout[m
[32m+[m[32m        self.drop = nn.Dropout(p=dropout)[m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m
[32m+[m[32m        # Concatenate flipped sequence on batch dim[m
[32m+[m[32m        if self.bidirectional:[m
[32m+[m[32m            x_flip = x.flip(1)[m
[32m+[m[32m            x = torch.cat([x, x_flip], dim=0)[m
[32m+[m
[32m+[m[32m        # Change batch size if needed[m
[32m+[m[32m        if self.batch_size != x.shape[0]:[m
[32m+[m[32m            self.batch_size = x.shape[0][m
[32m+[m
[32m+[m[32m        # Feed-forward affine transformations (all steps in parallel)[m
[32m+[m[32m        Wx = self.W(x)[m
[32m+[m
[32m+[m[32m        #Wx = self.output_linear(Wx.reshape(Wx.shape[0], Wx.shape[2], Wx.shape[1])).reshape(Wx.shape[0], Wx.shape[1], Wx.shape[2])[m
[32m+[m
[32m+[m[32m        # Apply normalization[m
[32m+[m[32m        if self.normalize:[m
[32m+[m[32m            _Wx = self.norm(Wx.reshape(Wx.shape[0] * Wx.shape[1], Wx.shape[2]))[m
[32m+[m[32m            Wx = _Wx.reshape(Wx.shape[0], Wx.shape[1], Wx.shape[2])[m
[32m+[m
[32m+[m[32m        # Compute spikes via neuron dynamics[m
[32m+[m[32m        s = self._rf_cell(Wx)[m
[32m+[m
[32m+[m[32m        # Concatenate forward and backward sequences on feat dim[m
[32m+[m[32m        if self.bidirectional:[m
[32m+[m[32m            s_f, s_b = s.chunk(2, dim=0)[m
[32m+[m[32m            s_b = s_b.flip(1)[m
[32m+[m[32m            s = torch.cat([s_f, s_b], dim=2)[m
[32m+[m
[32m+[m[32m        # Apply dropout[m
[32m+[m[32m        s = self.drop(s)[m
[32m+[m
[32m+[m[32m        return s[m
[32m+[m
[32m+[m[32m    def _rf_cell(self, Wx):[m
[32m+[m
[32m+[m[32m        # Initializations[m
[32m+[m[32m        device = Wx.device[m
[32m+[m[32m        ut = torch.rand(Wx.shape[0], Wx.shape[2], dtype=torch.cfloat).to(device)[m
[32m+[m[32m        st = torch.rand(Wx.shape[0], Wx.shape[2]).to(device)[m
[32m+[m
[32m+[m[32m        s = [][m[41m       [m
[32m+[m
[32m+[m[32m        if self.recurrent:[m
[32m+[m[32m            V = self.V.weight.clone().fill_diagonal_(0)[m
[32m+[m
[32m+[m[32m        if self.reparam or self.s4_init:[m
[32m+[m[32m            alpha = torch.exp((-torch.exp(self.alpha_real)+1j*self.alpha_im)*torch.exp(self.dt))[m
[32m+[m[32m        elif self.continuous or self.dt_train:[m
[32m+[m[32m            dt = torch.clamp(self.dt, min = 0.0004, max=1.0)[m
[32m+[m[32m            alpha_real = torch.clamp(self.alpha_real, min = 0.1)[m
[32m+[m[32m            alpha = torch.exp((-alpha_real+1j*self.alpha_im)*dt)[m
[32m+[m[32m        elif self.taylor:[m
[32m+[m[32m            alpha_real = torch.clamp(self.alpha_real, min = 0.1)[m
[32m+[m[32m            alpha = 1 + (-alpha_real+1j*self.alpha_im)*self.dt[m
[32m+[m[32m        else:[m
[32m+[m[32m            alpha_real = torch.clamp(self.alpha_real, min = 0.3, max=0.99)[m
[32m+[m[32m            alpha = alpha_real + 1j*self.alpha_im[m
[32m+[m
[32m+[m[32m        # Loop over time axis[m
[32m+[m[32m        for t in range(Wx.shape[1]):[m
[32m+[m
[32m+[m[32m            if self.recurrent:[m
[32m+[m[32m                I = Wx[:, t, :] + torch.matmul(st, V)[m
[32m+[m[32m            else:[m
[32m+[m[32m                I = Wx[:, t, :][m
[32m+[m[32m            # Compute membrane potential (LIF)[m
[32m+[m
[32m+[m[32m            ut = alpha*(ut - self.reset_factor*st) + self.b * I[m
[32m+[m
[32m+[m[32m            # Compute spikes with surrogate gradient[m
[32m+[m[32m            st = self.spike_fct((1/self.reset_factor)*ut.real - self.threshold)[m
[32m+[m[32m            s.append(st)[m
[32m+[m
[32m+[m[32m        return torch.stack(s, dim=1)[m
[32m+[m
[32m+[m[32m    def register(self, name, tensor, lr=None):[m
[32m+[m[32m        """Register a tensor with a configurable learning rate and 0 weight decay"""[m
[32m+[m
[32m+[m[32m        if lr == 0.0:[m
[32m+[m[32m            self.register_buffer(name, tensor)[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.register_parameter(name, nn.Parameter(tensor))[m
[32m+[m
[32m+[m[32m            optim = {"weight_decay": 0.0}[m
[32m+[m[32m            if lr is not None: optim["lr"] = lr[m
[32m+[m[32m            setattr(getattr(self, name), "_optim", optim)[m
[32m+[m
 class ResonateFireLayer(nn.Module):[m
     """[m
     A single layer of Leaky Integrate-and-Fire neurons without layer-wise[m
[1mdiff --git a/sparch/parsers/model_config.py b/sparch/parsers/model_config.py[m
[1mindex 6d4b05e..a1b32f9 100644[m
[1m--- a/sparch/parsers/model_config.py[m
[1m+++ b/sparch/parsers/model_config.py[m
[36m@@ -30,7 +30,7 @@[m [mdef add_model_options(parser):[m
     parser.add_argument([m
         "--model_type",[m
         type=str,[m
[31m-        choices=["LIF", "LIFfeature", "adLIFnoClamp", "LIFfeatureDim", "adLIF", "CadLIF", "BRF", "ResonateFire", "RSEadLIF", "RLIF", "RadLIF", "MLP", "RNN", "LiGRU", "GRU","LIFcomplexBroad", "LIFcomplex", "LIFrealcomplex", "ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlpha", "adLIFclamp", "RLIFcomplex1MinAlphaNoB","LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"],[m
[32m+[m[32m        choices=["LIF", "LIFfeature", "adLIFnoClamp", "LIFfeatureDim", "adLIF", "CadLIF", "RAFAblation", "BRF", "ResonateFire", "RSEadLIF", "RLIF", "RadLIF", "MLP", "RNN", "LiGRU", "GRU","LIFcomplexBroad", "LIFcomplex", "LIFrealcomplex", "ReLULIFcomplex", "RLIFcomplex","RLIFcomplex1MinAlpha", "adLIFclamp", "RLIFcomplex1MinAlphaNoB","LIFcomplex_gatedB", "LIFcomplex_gatedDt", "LIFcomplexDiscr"],[m
         default="LIF",[m
         help="Type of ANN or SNN model.",[m
     )[m
[36m@@ -56,6 +56,41 @@[m [mdef add_model_options(parser):[m
         default=[False],[m
         help="Use a recurrent version of the model.",[m
     )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--taylor",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--continuous",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--reparam",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--dt_train",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--s4_init",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
     parser.add_argument([m
         "--exp_factor",[m
         nargs='+',[m
[36m@@ -105,6 +140,13 @@[m [mdef add_model_options(parser):[m
         default=[False],[m
         help="Use (1 - alpha) to gate input instead of b for LIFcomplex.",[m
     )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--extra_b",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=str2bool,[m
[32m+[m[32m        default=[False],[m
[32m+[m[32m        help="Use half reset for LIFcomplex and RLIFcomplex models. True by default",[m
[32m+[m[32m    )[m
     parser.add_argument([m
         "--c_sum",[m
         nargs='+',[m
[36m@@ -169,7 +211,7 @@[m [mdef add_model_options(parser):[m
     parser.add_argument([m
         "--dt_max",[m
         type=float,[m
[31m-        default=[0.7],[m
[32m+[m[32m        default=[0.5],[m
         nargs='+',[m
         help="Max dt initializationfor LIFcomplex ",[m
     )[m
[1mdiff --git a/sparch/parsers/training_config.py b/sparch/parsers/training_config.py[m
[1mindex cf680cc..689f676 100644[m
[1m--- a/sparch/parsers/training_config.py[m
[1m+++ b/sparch/parsers/training_config.py[m
[36m@@ -184,6 +184,14 @@[m [mdef add_training_options(parser):[m
         help="Number of epochs without progress before the learning rate "[m
         "gets decreased.",[m
     )[m
[32m+[m[32m    parser.add_argument([m
[32m+[m[32m        "--num_workers",[m
[32m+[m[32m        nargs='+',[m
[32m+[m[32m        type=int,[m
[32m+[m[32m        default=[8],[m
[32m+[m[32m        help="Number of epochs without progress before the learning rate "[m
[32m+[m[32m        "gets decreased.",[m
[32m+[m[32m    )[m
     parser.add_argument([m
         "--scheduler_factor",[m
         type=float,[m
